{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "304e7b62-f9ac-40b4-baf2-08635b118f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#将fna文件内容转化为序列\n",
    "output_file_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics/1.txt'\n",
    "\n",
    "with open('/home/nusri/下载/dna_feat/dataset/probiotics_fna/GCF_002356135.1_ASM235613v1_genomic.fna', 'r') as seq:\n",
    "    lines = seq.readlines()\n",
    "    part = ''\n",
    "    for line in lines[1:]:\n",
    "        part += line.strip()\n",
    "        \n",
    "    with open(output_file_path, 'w') as output_file:\n",
    "        output_file.write(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0ebb0a-eef5-4bdd-a2ba-7859a93da252",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/home/nusri/下载/dna_feat/dataset/nonprobiotics_fna'\n",
    "\n",
    "output_file_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics'\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        with open(file_path, 'w') as seq:\n",
    "            lines = seq.lines[1:]\n",
    "            line.strip()\n",
    "            \n",
    "            output_file.write(lines.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5875685c-6629-4e58-a21d-2554abbdfd8c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_io.TextIOWrapper' object has no attribute 'lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# with open(output_file_path, 'w') as output_file:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     for filename in os.listdir(folder_path):\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#         file_path = os.path.join(folder_path, filename)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m seq:\n\u001b[0;32m---> 13\u001b[0m     lines \u001b[38;5;241m=\u001b[39m seq\u001b[38;5;241m.\u001b[39mlines[\u001b[38;5;241m1\u001b[39m:]  \n\u001b[1;32m     14\u001b[0m     output_file\u001b[38;5;241m.\u001b[39mwrite(lines\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_io.TextIOWrapper' object has no attribute 'lines'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = '/home/nusri/下载/dna_feat/dataset/nonprobiotics_fna'\n",
    "\n",
    "output_file_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics'\n",
    "\n",
    "file_path = '/home/nusri/下载/dna_feat/dataset/nonprobiotics_fna/GCA_002211645.1_ASM221164v1_genomic.fna'\n",
    "\n",
    "file_name = ''\n",
    "# with open(output_file_path, 'w') as output_file:\n",
    "#     for filename in os.listdir(folder_path):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "with open(file_path, 'w') as output_file:\n",
    "    lines = seq.lines[1:]  \n",
    "    output_file.write(lines.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7071274d-9aad-4c0a-811a-06b6b883ade9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db464fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def combine_DNA_seq(folder_path, result_folder_path):\n",
    "    os.makedirs(result_folder_path, exist_ok=True)\n",
    "    dirs = [path for path in os.listdir(folder_path) if path.endswith('fna')]\n",
    "    for input_file in tqdm(dirs):\n",
    "        with open(os.path.join(folder_path, input_file), 'r') as seq:\n",
    "            file_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "            lines = seq.readlines()\n",
    "            part = ''\n",
    "            for line in lines[1:]:\n",
    "                if line[0] == '>':\n",
    "                    break\n",
    "                part += line.strip()\n",
    "\n",
    "            \n",
    "            output_file_path = os.path.join(result_folder_path, f'{file_name}.txt')\n",
    "            with open(output_file_path, 'w') as output_file:\n",
    "                output_file.write(part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a94945c4-5c7e-4102-895d-665c0897ea60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405/405 [00:08<00:00, 48.82it/s]\n"
     ]
    }
   ],
   "source": [
    "#创建nonprobiotics的序列文档\n",
    "folder_path = '/home/nusri/下载/dna_feat/dataset/nonprobiotics_fna'\n",
    "result_folder_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics'\n",
    "\n",
    "combine_DNA_seq(folder_path, result_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1660c368-c4b5-4ad8-b537-f886f0e92f8b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 238/238 [00:02<00:00, 91.21it/s] \n"
     ]
    }
   ],
   "source": [
    "#创建probiotics的序列文档\n",
    "folder_path = '/home/nusri/下载/dna_feat/dataset/probiotics_fna'\n",
    "result_folder_path = '/home/nusri/下载/dna_feat/dataset/merged_probiotics'\n",
    "\n",
    "combine_DNA_seq(folder_path, result_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f2937e-6738-4fa4-894f-8a51705e5cbd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def generate_seq(k, chars, current_string, results):\n",
    "    if k == 0:\n",
    "        results.append(current_string)\n",
    "        return\n",
    "    for char in chars:\n",
    "        generate_seq(k - 1, chars, current_string + char, results)\n",
    "\n",
    "#生成所有k-mer序列\n",
    "def all_possible_seq(k, chars):\n",
    "    results = []\n",
    "    generate_seq(k, chars, '', results)\n",
    "    return results\n",
    "\n",
    "dna_list = ['A', 'G', 'T', 'C']\n",
    "\n",
    "with open('/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics/2.txt', 'r') as seqs:\n",
    "    dna_seq = seqs.read()\n",
    "\n",
    "#创建kmer序列与对应tf的字典\n",
    "def get_dict(seq):\n",
    "    curr_dict = {}\n",
    "    for k in range(2, 7):\n",
    "        dna_segs = all_possible_seq(k, dna_list)      #k-mer序列\n",
    "        new_dict = {seg:0 for seg in dna_segs}        #创建一个初始字典，key为k-mer序列，value为0\n",
    "        curr_dict.update(new_dict)                    #在k值更新之后更新字典\n",
    "        all_occur = len(seq) - k + 1                  #k-mer序列的数量\n",
    "        for x in range(all_occur):                    #计算每个k-mer序列的出现次数\n",
    "            key = seq[x: x+k]                         \n",
    "            curr_dict[key] += 1\n",
    "        curr_dict = {k:v for k,v in curr_dict.items()}#字典储存在curr_dict \n",
    "\n",
    "    for key in curr_dict.keys():                      \n",
    "        l = len(seq) - len(key) + 1\n",
    "        curr_dict[key] = curr_dict[key] / l           #计算tf\n",
    "    return(curr_dict)\n",
    "\n",
    "def feat2str(int_float_dict):\n",
    "    feat_str = ''\n",
    "    for index, value in int_float_dict.items():\n",
    "        feat_str += str(index) + ':' + str(value) + ' '  \n",
    "    #feat_str = str(label) + '\\t' + feat_str \n",
    "    return feat_str\n",
    "    \n",
    "def main(seq):\n",
    "    res_dict = ''\n",
    "    feat_dict = get_dict(seq)\n",
    "    #给kmer序列标号并创建字典\n",
    "    index_ATCG_map = {ATCG: int(i) for i, ATCG in enumerate(feat_dict.keys())}\n",
    "    #创建index和tf的字典\n",
    "    feat_dict = {index_ATCG_map[ATCG]: value for i, (ATCG, value) in enumerate(feat_dict.items())}\n",
    "    res_dict += feat2str(feat_dict)\n",
    "    return res_dict\n",
    "\n",
    "main(dna_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b61f54a1-b31b-4569-87c6-beb32e743c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "#生成所有k-mer序列\n",
    "dna_list = ['A', 'G', 'T', 'C']\n",
    "\n",
    "def all_possible_seq(k):\n",
    "    \n",
    "    def generate_seq(k, chars, current_string, results):\n",
    "        if k == 0:\n",
    "            results.append(current_string)\n",
    "            return\n",
    "        for char in chars:\n",
    "            generate_seq(k - 1, chars, current_string + char, results)\n",
    "\n",
    "    results = []\n",
    "    generate_seq(k, dna_list, '', results)\n",
    "    return results\n",
    "\n",
    "ambiguity_symbol_dict = {\n",
    "    'A': ['A'], 'T': ['T'], 'C': ['C'],'G': ['G'],\n",
    "    'W': ['A', 'T'], 'S': ['C', 'G'], \n",
    "    'M': ['A', 'C'], 'K': ['G', 'T'], \n",
    "    'R': ['A', 'G'], 'Y': ['C', 'T'],\n",
    "    'B': ['C', 'G', 'T'], 'D': ['A', 'G', 'T'], \n",
    "    'H': ['A', 'C', 'T'], 'V': ['A', 'C', 'G'], \n",
    "    'N': ['A', 'C', 'G', 'T']\n",
    "}\n",
    "\n",
    "def nucleic_acid_count(dna_seg):\n",
    "    result = {'A': 0.0, 'T': 0.0, 'C': 0.0, 'G': 0.0}\n",
    "\n",
    "    for char in dna_seg:\n",
    "        possib_chars = ambiguity_symbol_dict[char]\n",
    "        possib_chars_num = len(possib_chars)\n",
    "        # e.g. 'W':['A', 'T'] --> 'A':0.5, 'T':0.5\n",
    "        #      'D':['A', 'G', 'T'] --> 'A':1/3, 'G':1/3, 'T':1/3\n",
    "        #      'A':['A'] --> 'A':1/1\n",
    "        for pc in possib_chars:\n",
    "            result[pc] += 1/possib_chars_num\n",
    "        \n",
    "    return Counter(result)\n",
    "\n",
    "# Test \n",
    "# nucleic_acid_count('AWCBN')\n",
    "\n",
    "#创建kmer序列与对应tf的字典\n",
    "def get_dict(seq):\n",
    "    curr_dict = Counter()\n",
    "    for k in range(2, 5):\n",
    "        dna_segs = all_possible_seq(k)                #k-mer序列\n",
    "        new_dict = {seg:0 for seg in dna_segs}        #创建一个初始字典，key为k-mer序列，value为0\n",
    "        curr_dict += new_dict                         #在k值更新之后更新字典\n",
    "        all_occur = len(seq) - k + 1                  #k-mer序列的数量\n",
    "        for x in range(all_occur):                    #计算每个k-mer序列的出现次数\n",
    "            key = seq[x: x+k]                         \n",
    "            # curr_dict[key] += 1\n",
    "            # curr_dict.update({ key: curr_dict[key]+1 })\n",
    "            # curr_dict.update({ key1: curr_dict[key]+0.5, key2: curr_dict[key]+0.5 })\n",
    "            # curr_dict.update({ lambda N: keyN: curr_dict[key]+1/N })\n",
    "            curr_dict += nucleic_acid_count(dna_seg=key)\n",
    "        \n",
    "        # curr_dict = {k:v for k,v in curr_dict.items()}#字典储存在curr_dict \n",
    "\n",
    "    for key in curr_dict.keys():                      \n",
    "        l = len(seq) - len(key) + 1\n",
    "        curr_dict[key] = curr_dict[key] / l           #计算tf\n",
    "    return(curr_dict)\n",
    "\n",
    "def feat2str(int_float_dict):\n",
    "    feat_str = ''\n",
    "    for index, value in int_float_dict.items():\n",
    "        feat_str += str(index) + ':' + str(value) + ' '  \n",
    "    #feat_str = str(label) + '\\t' + feat_str \n",
    "    return feat_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c52f511c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [05:43, 24.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m feat_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     feat_dict \u001b[39m=\u001b[39m get_dict(seq)\n\u001b[1;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mERROR: \u001b[39m\u001b[39m'\u001b[39m, filename)\n",
      "Cell \u001b[0;32mIn[9], line 62\u001b[0m, in \u001b[0;36mget_dict\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     57\u001b[0m         key \u001b[39m=\u001b[39m seq[x: x\u001b[39m+\u001b[39mk]                         \n\u001b[1;32m     58\u001b[0m         \u001b[39m# curr_dict[key] += 1\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[39m# curr_dict.update({ key: curr_dict[key]+1 })\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[39m# curr_dict.update({ key1: curr_dict[key]+0.5, key2: curr_dict[key]+0.5 })\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[39m# curr_dict.update({ lambda N: keyN: curr_dict[key]+1/N })\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m         curr_dict \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nucleic_acid_count(dna_seg\u001b[39m=\u001b[39mkey)\n\u001b[1;32m     64\u001b[0m     \u001b[39m# curr_dict = {k:v for k,v in curr_dict.items()}#字典储存在curr_dict \u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m curr_dict\u001b[39m.\u001b[39mkeys():                      \n",
      "Cell \u001b[0;32mIn[9], line 41\u001b[0m, in \u001b[0;36mnucleic_acid_count\u001b[0;34m(dna_seg)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[39m# e.g. 'W':['A', 'T'] --> 'A':0.5, 'T':0.5\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[39m#      'D':['A', 'G', 'T'] --> 'A':1/3, 'G':1/3, 'T':1/3\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     \u001b[39m#      'A':['A'] --> 'A':1/1\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m pc \u001b[39min\u001b[39;00m possib_chars:\n\u001b[0;32m---> 41\u001b[0m         result[pc] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\u001b[39m/\u001b[39mpossib_chars_num\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m Counter(result)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics'\n",
    "label = 0\n",
    "dirs = [path for path in os.listdir(folder_path) if path.endswith('txt')]\n",
    "seq_list = []\n",
    "filename_list =[]\n",
    "# for input_file in dirs[:3]:\n",
    "for input_file in dirs:\n",
    "    with open(os.path.join(folder_path, input_file)) as seqs:\n",
    "        dna_seq = seqs.read()\n",
    "        seq_list.append(dna_seq)\n",
    "        filename_list.append(input_file)\n",
    "\n",
    "\n",
    "feat_str_list =[]\n",
    "for seq, filename in tqdm(zip(seq_list,filename_list),total=len(seq_list)):\n",
    "    feat_str = ''\n",
    "    try:\n",
    "        feat_dict = get_dict(seq)\n",
    "    except Exception as e:\n",
    "        print('ERROR: ', filename)\n",
    "        raise('ERROR')\n",
    "    #给kmer序列标号并创建字典\n",
    "    index_ATCG_map = {ATCG: int(i) for i, ATCG in enumerate(feat_dict.keys())}\n",
    "    #创建index和tf的字典\n",
    "    feat_dict = {index_ATCG_map[ATCG]: value for i, (ATCG, value) in enumerate(feat_dict.items())}\n",
    "    feat_str += feat2str(feat_dict)\n",
    "    feat_str_list.append(feat_str)\n",
    "\n",
    "with open('result_nonprobio.tsv', 'w') as f:\n",
    "    for feat_str in feat_str_list:\n",
    "        f.write(label + '\\t' + feat_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa422b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [1:16:19, 117.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m feat_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m     feat_dict \u001b[39m=\u001b[39m get_dict(seq)\n\u001b[1;32m     19\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     20\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mERROR: \u001b[39m\u001b[39m'\u001b[39m, filename)\n",
      "Cell \u001b[0;32mIn[6], line 62\u001b[0m, in \u001b[0;36mget_dict\u001b[0;34m(seq)\u001b[0m\n\u001b[1;32m     57\u001b[0m         key \u001b[39m=\u001b[39m seq[x: x\u001b[39m+\u001b[39mk]                         \n\u001b[1;32m     58\u001b[0m         \u001b[39m# curr_dict[key] += 1\u001b[39;00m\n\u001b[1;32m     59\u001b[0m         \u001b[39m# curr_dict.update({ key: curr_dict[key]+1 })\u001b[39;00m\n\u001b[1;32m     60\u001b[0m         \u001b[39m# curr_dict.update({ key1: curr_dict[key]+0.5, key2: curr_dict[key]+0.5 })\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[39m# curr_dict.update({ lambda N: keyN: curr_dict[key]+1/N })\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m         curr_dict \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m nucleic_acid_count(dna_seg\u001b[39m=\u001b[39mkey)\n\u001b[1;32m     64\u001b[0m     \u001b[39m# curr_dict = {k:v for k,v in curr_dict.items()}#字典储存在curr_dict \u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m curr_dict\u001b[39m.\u001b[39mkeys():                      \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "folder_path = '/home/nusri/下载/dna_feat/dataset/merged_nonprobiotics'\n",
    "label = 0\n",
    "dirs = [path for path in os.listdir(folder_path) if path.endswith('txt')]\n",
    "seq_list = []\n",
    "filename_list =[]\n",
    "# for input_file in dirs[:3]:\n",
    "for input_file in dirs:\n",
    "    with open(os.path.join(folder_path, input_file)) as seqs:\n",
    "        dna_seq = seqs.read()\n",
    "        seq_list.append(dna_seq)\n",
    "        filename_list.append(input_file)\n",
    "\n",
    "\n",
    "feat_str_list =[]\n",
    "for seq, filename in tqdm(zip(seq_list,filename_list)):\n",
    "    feat_str = ''\n",
    "    try:\n",
    "        feat_dict = get_dict(seq)\n",
    "    except Exception as e:\n",
    "        print('ERROR: ', filename)\n",
    "        raise('ERROR')\n",
    "    #给kmer序列标号并创建字典\n",
    "    index_ATCG_map = {ATCG: int(i) for i, ATCG in enumerate(feat_dict.keys())}\n",
    "    #创建index和tf的字典\n",
    "    feat_dict = {index_ATCG_map[ATCG]: value for i, (ATCG, value) in enumerate(feat_dict.items())}\n",
    "    feat_str += feat2str(feat_dict)\n",
    "    feat_str_list.append(feat_str)\n",
    "\n",
    "with open('result_nonprobio.tsv', 'w') as f:\n",
    "    for feat_str in feat_str_list:\n",
    "        f.write(label + '\\t' + feat_str + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ed3945-e7d8-4412-a1f5-05d572737ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints', 'dataset/nonprobiotics_fna/.ipynb_checkpoints']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob\n",
    "list(glob('**/.ipynb_checkpoints', recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee3d77d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
